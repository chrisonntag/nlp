{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Exercise Sheet 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for all exercises\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk.classify as classify\n",
    "from string import ascii_lowercase\n",
    "from itertools import cycle\n",
    "\n",
    "from nltk.corpus import names\n",
    "from nltk.corpus import senseval\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Write a name gender classifier using the Names Corpus, the `apply_features` function, shuffling, and a test set of 500 instances. Use the following features:\n",
    "\n",
    "a) first letter;  \n",
    "b) last letter;  \n",
    "c) last two letters;  \n",
    "d) length;  \n",
    "e) for each letter one feature, which is true if the name contains the letter.\n",
    "\n",
    "Use the `NaiveBayesClassifier`, calculate the accuracy, and display the 10 most informative features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(name):\n",
    "    feature_dict = {\n",
    "        'word': name,\n",
    "        'first_letter': name[0],\n",
    "        'last_letter': name[-1:],\n",
    "        'last_two_letters': name[-2:],\n",
    "        'length': len(name)\n",
    "    }\n",
    "    \n",
    "    for char in ascii_lowercase:\n",
    "        feature_dict[char] = True if char in name else False\n",
    "        \n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_names = ([(name.lower(), 'male') for name in names.words('male.txt')] + [(name.lower(), 'female') for name in names.words('female.txt')])\n",
    "random.shuffle(labeled_names)\n",
    "\n",
    "train_set, test_set = classify.apply_features(gender_features, labeled_names[:-500]), \\\n",
    "                      classify.apply_features(gender_features, labeled_names[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.76\n",
      "\n",
      "Most Informative Features\n",
      "        last_two_letters = 'na'           female : male   =    167.1 : 1.0\n",
      "        last_two_letters = 'la'           female : male   =     74.4 : 1.0\n",
      "        last_two_letters = 'ia'           female : male   =     56.7 : 1.0\n",
      "             last_letter = 'a'            female : male   =     38.4 : 1.0\n",
      "        last_two_letters = 'sa'           female : male   =     34.3 : 1.0\n",
      "             last_letter = 'k'              male : female =     31.6 : 1.0\n",
      "        last_two_letters = 'rd'             male : female =     31.6 : 1.0\n",
      "        last_two_letters = 'us'             male : female =     29.7 : 1.0\n",
      "        last_two_letters = 'ra'           female : male   =     25.9 : 1.0\n",
      "        last_two_letters = 'ta'           female : male   =     25.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", classify.accuracy(classifier, test_set), end=\"\\n\\n\")\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "The Senseval 2 Corpus contains data intended to train word-sense disambiguation classifiers. Using this dataset, build a `NaiveBayesClassifier` that predicts the correct sense tag for a given instance for the word \"hard\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SensevalInstance(word='hard-a', position=20, context=[('``', '``'), ('he', 'PRP'), ('may', 'MD'), ('lose', 'VB'), ('all', 'DT'), ('popular', 'JJ'), ('support', 'NN'), (',', ','), ('but', 'CC'), ('someone', 'NN'), ('has', 'VBZ'), ('to', 'TO'), ('kill', 'VB'), ('him', 'PRP'), ('to', 'TO'), ('defeat', 'VB'), ('him', 'PRP'), ('and', 'CC'), ('that', 'DT'), (\"'s\", 'VBZ'), ('hard', 'JJ'), ('to', 'TO'), ('do', 'VB'), ('.', '.'), (\"''\", \"''\")], senses=('HARD1',))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances = senseval.instances('hard.pos')\n",
    "labeled_instances = [(inst, inst.senses) for inst in instances]\n",
    "\n",
    "tags_set = set()\n",
    "for instance in instances:\n",
    "    for el in instance.context:\n",
    "        tags_set.add(el[1])\n",
    "        \n",
    "instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(tag, taglist):\n",
    "    return sum([1 if t==tag else 0 for t in taglist])\n",
    "\n",
    "def gen_senseval_features(instance):\n",
    "    instance_tags = [c[1] for c in instance.context]\n",
    "        \n",
    "    feature_dict = {\n",
    "        \"position\": instance.position,\n",
    "        #\"position_ratio\": instance.position / len(instance.context),\n",
    "        \"context\": \" \".join(instance_tags)\n",
    "    }\n",
    "    \n",
    "    if instance.position > 0:\n",
    "        feature_dict[\"preceding\"] = instance.context[instance.position - 1]\n",
    "    else:\n",
    "        feature_dict[\"preceding\"] = None\n",
    "    \n",
    "    if instance.position < len(instance.context):\n",
    "        feature_dict[\"succeeding\"] = instance.context[instance.position + 1]\n",
    "    else:\n",
    "        feature_dict[\"succeeding\"] = None\n",
    "        \n",
    "    \"\"\"\n",
    "    # Every possible tag count\n",
    "    for tag in tags_set:\n",
    "        feature_dict[tag] = count(tag, instance_tags)\n",
    "    \n",
    "    # Every possible tag True/False\n",
    "    for tag in tags_set:\n",
    "        feature_dict[tag] = True if tag in instance_tags else False\n",
    "    \"\"\"\n",
    "    \n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test(data, feature_func, test_ratio=0.1, shuffle=True):\n",
    "    if shuffle:\n",
    "        random.shuffle(data)\n",
    "    test_size = int(len(data) * test_ratio)\n",
    "\n",
    "    train_set = classify.apply_features(feature_func, data[test_size:])\n",
    "    test_set = classify.apply_features(feature_func, data[:test_size])\n",
    "    \n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "train_set, test_set = prepare_train_test(labeled_instances, gen_senseval_features, shuffle=True)\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8637413394919169\n",
      "\n",
      "Most Informative Features\n",
      "              succeeding = ('to', 'TO')    HARD1 : HARD2  =    139.4 : 1.0\n",
      "               preceding = (\"'s\", 'VBZ')   HARD1 : HARD3  =     71.6 : 1.0\n",
      "              succeeding = ('work', 'NN')  HARD2 : HARD3  =     71.3 : 1.0\n",
      "              succeeding = ('line', 'NN')  HARD2 : HARD1  =     52.4 : 1.0\n",
      "              succeeding = ('place', 'NN')  HARD3 : HARD1  =     44.9 : 1.0\n",
      "               preceding = ('no', 'DT')    HARD2 : HARD1  =     35.9 : 1.0\n",
      "              succeeding = ('for', 'IN')   HARD1 : HARD2  =     34.9 : 1.0\n",
      "              succeeding = ('evidence', 'NN')  HARD2 : HARD1  =     25.3 : 1.0\n",
      "               preceding = ('other', 'JJ')  HARD3 : HARD1  =     20.7 : 1.0\n",
      "              succeeding = (\"''\", \"''\")    HARD3 : HARD1  =     19.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", classify.accuracy(classifier, test_set), end=\"\\n\\n\")\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the preceding and following word as features. They can be calculated by retrieving the position of the word \"hard\" as `p=inst.position` and then accessing `inst.context[p-1]` and `inst.context[p+1]`.\n",
    "\n",
    "Run 10 iterations by reshuffling the instances and printing the individual accuracies. Finally, print the average accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on iteration 1: 0.909931\n",
      "Accuracy on iteration 2: 0.898383\n",
      "Accuracy on iteration 3: 0.928406\n",
      "Accuracy on iteration 4: 0.868360\n",
      "Accuracy on iteration 5: 0.882217\n",
      "Accuracy on iteration 6: 0.882217\n",
      "Accuracy on iteration 7: 0.898383\n",
      "Accuracy on iteration 8: 0.879908\n",
      "Accuracy on iteration 9: 0.893764\n",
      "Accuracy on iteration 10: 0.898383\n",
      "\n",
      "Average accuracy:  0.8939953810623557\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for i in range(1, 11):\n",
    "    train_set, test_set = prepare_train_test(labeled_instances, gen_senseval_features, shuffle=True)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    accuracy = classify.accuracy(classifier, test_set)\n",
    "    accuracies.append(accuracy)\n",
    "    print(\"Accuracy on iteration %d: %f\" % (i, accuracy))\n",
    "    \n",
    "print(\"\\nAverage accuracy: \", sum(accuracies)/len(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "The synonyms \"strong\" and \"powerful\" pattern differently. Use the tagged Brown corpus with the universal tagset to first list the nouns which follow \"strong\" vs. \"powerful\". Write for this a function `next_noun(word, tagged_text)` which returns the list of nouns that follow `word` in the `tagged_text`. Build then a `NaiveBayesClassifier` that predicts when each word should be used by using the function `apply_features` and the following noun as single feature.\n",
    "\n",
    "Run 10 iterations by reshuffling the instances and printing the individual accuracies. Finally, print the average accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_noun(word: str, tagged_text: list[tuple[str, str]]):\n",
    "    actively_looking = False # avoids second for loop\n",
    "    nouns = []\n",
    "    for w, t in tagged_text:\n",
    "        if w == word:\n",
    "            actively_looking = True\n",
    "            \n",
    "            # This word is not a noun\n",
    "            continue\n",
    "            \n",
    "        if actively_looking:\n",
    "            if t == 'NOUN':\n",
    "                nouns.append(w)\n",
    "                actively_looking = False\n",
    "                \n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = brown.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_adj = list(zip(next_noun(\"strong\", tagged_words), cycle([\"strong\"]))) + list(zip(next_noun(\"powerful\", tagged_words), cycle([\"powerful\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noun_features(noun):\n",
    "    return {'noun': noun}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on iteration 1: 0.640000\n",
      "Accuracy on iteration 2: 0.640000\n",
      "Accuracy on iteration 3: 0.880000\n",
      "Accuracy on iteration 4: 0.800000\n",
      "Accuracy on iteration 5: 0.800000\n",
      "Accuracy on iteration 6: 0.800000\n",
      "Accuracy on iteration 7: 0.760000\n",
      "Accuracy on iteration 8: 0.880000\n",
      "Accuracy on iteration 9: 0.720000\n",
      "Accuracy on iteration 10: 0.680000\n",
      "\n",
      "Average accuracy:  0.7599999999999999\n"
     ]
    }
   ],
   "source": [
    "best_classifier = (0.0, None)\n",
    "accuracies = []\n",
    "for i in range(1, 11):\n",
    "    train_set, test_set = prepare_train_test(labeled_adj, gen_noun_features, shuffle=True)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    accuracy = classify.accuracy(classifier, test_set)\n",
    "    if accuracy > best_classifier[0]:\n",
    "        best_classifier = (accuracy, classifier)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    print(\"Accuracy on iteration %d: %f\" % (i, accuracy))\n",
    "    \n",
    "print(\"\\nAverage accuracy: \", sum(accuracies)/len(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'powerful'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier[1].classify(gen_noun_features('nature'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Based on the Movie Reviews document classifier discussed in this chapter, build a new `NaiveBayesClassifier`. Tag first the Movie Reviews Corpus using the combined tagger from the previous chapter stored in `t2.pkl`. Filter the tagged words to contain only words for the tags `['JJ', 'JJR', 'JJS', 'RB', 'NN', 'NNS', 'VB', 'VBN', 'VBG', 'VBZ', 'VBD', 'QL']` as well as only alphabetic tokens with at least three characters. Convert the words to lowercase. Use the most common 5000 words as `word_features` in the function `document_features`. \n",
    "\n",
    "Run 10 iterations by reshuffling the instances and printing the accuracy and 5 most informative features for each iteration. Finally, print the average accuracy.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452108043456593"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy from chapter 5\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "t2.evaluate(test_sents)\n",
    "# End copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_tagged = t2.tag(movie_reviews.words())\n",
    "reviews_tagged = [(word.lower(), tag) for (word, tag) in reviews_tagged \\\n",
    "                  if tag in ['JJ', 'JJR', 'JJS', 'RB', 'NN', 'NNS', 'VB', 'VBN', 'VBG', 'VBZ', 'VBD', 'QL'] \\\n",
    "                  and len(word) >= 3 and word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdst = nltk.FreqDist(reviews_tagged)\n",
    "word_features = [tagged_word[0] for tagged_word, freq in fdst.most_common(5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to nltk book\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category) \\\n",
    "             for category in movie_reviews.categories() \\\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.785\n",
      "Most Informative Features\n",
      "   contains(outstanding) = True              pos : neg    =     12.3 : 1.0\n",
      "     contains(stupidity) = True              neg : pos    =     11.5 : 1.0\n",
      "         contains(stark) = True              pos : neg    =     11.0 : 1.0\n",
      "     contains(ludicrous) = True              neg : pos    =     10.5 : 1.0\n",
      "     contains(insulting) = True              neg : pos    =     10.2 : 1.0\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:  0.81\n",
      "Most Informative Features\n",
      "     contains(ludicrous) = True              neg : pos    =     12.8 : 1.0\n",
      "   contains(outstanding) = True              pos : neg    =     12.8 : 1.0\n",
      "        contains(hudson) = True              neg : pos    =      9.2 : 1.0\n",
      "     contains(insulting) = True              neg : pos    =      9.2 : 1.0\n",
      "         contains(lousy) = True              neg : pos    =      9.2 : 1.0\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:  0.84\n",
      "Most Informative Features\n",
      "   contains(outstanding) = True              pos : neg    =     12.6 : 1.0\n",
      "    contains(schumacher) = True              neg : pos    =     11.9 : 1.0\n",
      "         contains(sucks) = True              neg : pos    =     10.4 : 1.0\n",
      "     contains(insulting) = True              neg : pos    =     10.0 : 1.0\n",
      "       contains(idiotic) = True              neg : pos    =      9.5 : 1.0\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:  0.815\n",
      "Most Informative Features\n",
      "         contains(sucks) = True              neg : pos    =     15.4 : 1.0\n",
      "   contains(outstanding) = True              pos : neg    =     13.1 : 1.0\n",
      "        contains(hudson) = True              neg : pos    =     10.6 : 1.0\n",
      "     contains(ludicrous) = True              neg : pos    =     10.1 : 1.0\n",
      "     contains(insulting) = True              neg : pos    =      9.2 : 1.0\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:  0.81\n",
      "Most Informative Features\n",
      "     contains(ludicrous) = True              neg : pos    =     23.0 : 1.0\n",
      "   contains(magnificent) = True              pos : neg    =     11.8 : 1.0\n",
      "   contains(outstanding) = True              pos : neg    =     10.5 : 1.0\n",
      "        contains(hudson) = True              neg : pos    =      9.7 : 1.0\n",
      "     contains(insulting) = True              neg : pos    =      9.4 : 1.0\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:  0.78\n",
      "Most Informative Features\n",
      "     contains(insulting) = True              neg : pos    =     16.9 : 1.0\n",
      "         contains(sucks) = True              neg : pos    =     13.6 : 1.0\n",
      "   contains(outstanding) = True              pos : neg    =     10.0 : 1.0\n",
      "        contains(hudson) = True              neg : pos    =      9.6 : 1.0\n",
      "     contains(ludicrous) = True              neg : pos    =      9.2 : 1.0\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:  0.825\n",
      "Most Informative Features\n",
      "   contains(outstanding) = True              pos : neg    =     13.6 : 1.0\n",
      "        contains(seagal) = True              neg : pos    =     11.5 : 1.0\n",
      "         contains(poker) = True              pos : neg    =     11.1 : 1.0\n",
      "     contains(stupidity) = True              neg : pos    =     10.9 : 1.0\n",
      "     contains(insulting) = True              neg : pos    =     10.5 : 1.0\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:  0.83\n",
      "Most Informative Features\n",
      "     contains(insulting) = True              neg : pos    =     17.1 : 1.0\n",
      "     contains(maintains) = True              pos : neg    =     12.9 : 1.0\n",
      "        contains(seagal) = True              neg : pos    =     12.4 : 1.0\n",
      "      contains(captures) = True              pos : neg    =     12.1 : 1.0\n",
      "   contains(outstanding) = True              pos : neg    =     11.3 : 1.0\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:  0.745\n",
      "Most Informative Features\n",
      "     contains(insulting) = True              neg : pos    =     16.4 : 1.0\n",
      "      contains(chilling) = True              pos : neg    =     13.6 : 1.0\n",
      "        contains(seagal) = True              neg : pos    =     11.7 : 1.0\n",
      "     contains(stupidity) = True              neg : pos    =     10.6 : 1.0\n",
      "     contains(ludicrous) = True              neg : pos    =     10.5 : 1.0\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:  0.84\n",
      "Most Informative Features\n",
      "        contains(seagal) = True              neg : pos    =     12.8 : 1.0\n",
      "     contains(stupidity) = True              neg : pos    =     11.6 : 1.0\n",
      "         contains(sucks) = True              neg : pos    =     10.4 : 1.0\n",
      "     contains(ludicrous) = True              neg : pos    =     10.0 : 1.0\n",
      "   contains(outstanding) = True              pos : neg    =      9.7 : 1.0\n",
      "\n",
      "\n",
      "\n",
      "Average accuracy:  0.8080000000000002\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for i in range(1, 11):\n",
    "    random.shuffle(documents)\n",
    "    featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "    test_size = int(len(featuresets) * 0.1)\n",
    "    \n",
    "    train_set, test_set = featuresets[test_size:], featuresets[:test_size]\n",
    "\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    classifier.show_most_informative_features(5)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "print(\"Average accuracy: \", sum(accuracies)/len(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "The PP Attachment Corpus is a corpus describing prepositional phrase attachment decisions. Each instance in the training corpus is encoded as a `PPAttachment` object:\n",
    "\n",
    "    from nltk.corpus import ppattach\n",
    "    ppattach.attachments('training')\n",
    "    \n",
    "        [PPAttachment(sent='0', verb='join', noun1='board',\n",
    "            prep='as', noun2='director', attachment='V'),\n",
    "        PPAttachment(sent='1', verb='is', noun1='chairman',\n",
    "            prep='of', noun2='N.V.', attachment='N'),\n",
    "        ...]\n",
    "\n",
    "    inst = ppattach.attachments('training')[1]\n",
    "    (inst.noun1, inst.prep, inst.noun2)\n",
    "    \n",
    "        ('chairman', 'of', 'N.V.')\n",
    "\n",
    "In the same way, `ppattach.attachments('test')` accesses the test instances. Select only the instances where `inst.attachment` is `'N'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import ppattach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this sub-corpus, build a `NaiveBayesClassifier` that attempts to predict which preposition is used to connect a given pair of nouns. For example, given the pair of nouns \"team\" and \"researchers\", the classifier should predict the preposition \"of\". \n",
    "\n",
    "Write for this purpose a function `prepare_featuresets(subcorpus)`, where `subcorpus` is either the string \"training\" or \"test\" to return the training set or the test set. \n",
    "\n",
    "Print the achieved accuracy as well as the result of `classifier.classify({ 'noun1': 'team', 'noun2': 'researchers' })`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_featuresets(subcorpus: str):\n",
    "    return [({'noun1': inst.noun1, 'noun2': inst.noun2}, inst.prep) \\\n",
    "            for inst in ppattach.attachments(subcorpus) \\\n",
    "            if inst.attachment == 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5690032858707558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(prepare_featuresets(\"training\"))\n",
    "accuracy = nltk.classify.accuracy(classifier, prepare_featuresets(\"test\"))\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "classifier.classify({ 'noun1': 'team', 'noun2': 'researchers' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify({ 'noun1': 'school', 'noun2': 'loosers' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
